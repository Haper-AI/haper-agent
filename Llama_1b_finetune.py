from huggingface_hub import notebook_login
notebook_login()

import json
import pandas as pd
from datasets import Dataset
import random

json_path = "/content/drive/MyDrive/AI agent/all_emails.json"
with open(json_path, "r", encoding="utf-8") as f:
    data = json.load(f)

# éšæœºæ·»åŠ  category å­—æ®µ
for item in data:
    item["category"] = random.choice(["important", "non-important"])

# ä¿å­˜ä¸ºæ–°çš„ JSON æ–‡ä»¶
new_json_path = "/content/drive/MyDrive/AI agent/all_emails_with_category.json"
with open(new_json_path, "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

import json
import pandas as pd
from datasets import Dataset

# è¯»å–æ ‡æ³¨åçš„ JSON æ–‡ä»¶
json_path = "/content/drive/MyDrive/AI agent/all_emails_with_category.json"  # ä¿®æ”¹ä¸ºä½ çš„ JSON æ–‡ä»¶è·¯å¾„
with open(json_path, "r", encoding="utf-8") as f:
    data = json.load(f)

# è½¬æ¢æˆ Pandas DataFrame
df = pd.DataFrame(data)

# æ˜ å°„ category åˆ° 0/1
df["label"] = df["category"].map({"important": 1, "non-important": 0})

# æ‹¼æ¥å¤šè¾“å…¥å­—æ®µ
df["text"] = "From: " + df["From"] + "\nSubject: " + df["Subject"] + "\nDate: " + df["Date"] + "\nContent: " + df["Content"]

# åˆ é™¤ä¸å¿…è¦çš„åˆ—
df = df[["text", "label"]]

# è½¬æ¢æˆ Hugging Face Dataset æ ¼å¼
dataset = Dataset.from_pandas(df)

from transformers import AutoModelForCausalLM, AutoTokenizer

# ä¸‹è½½ LLaMA-1B
model_id = "meta-llama/Llama-3.2-1B"  # ç¡®ä¿ LLaMA-1B å¯ç”¨ï¼Œå¦åˆ™æ¢æˆ LLaMA-7B æˆ–æ›´å°çš„æ¨¡å‹


model = AutoModelForCausalLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

tokenizer.pad_token = tokenizer.eos_token

# Tokenize æ•°æ®
def preprocess_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

tokenized_datasets = dataset.map(preprocess_function, batched=True)

# è®­ç»ƒ/æµ‹è¯•é›†æ‹†åˆ†
train_test_split = tokenized_datasets.train_test_split(test_size=0.2)
train_dataset = train_test_split["train"]
test_dataset = train_test_split["test"]

print(test_dataset)
print(len(train_dataset[0]['input_ids']))
print(len(train_dataset[1]['input_ids']))

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EvalPrediction
from peft import LoraConfig, get_peft_model
from transformers import DataCollatorWithPadding
import numpy as np
from sklearn.metrics import accuracy_score, f1_score


# åŠ è½½ 4-bit é‡åŒ– LLaMA-1B
from transformers import BitsAndBytesConfig
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype="float16", bnb_4bit_use_double_quant=True)

model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    num_labels=2,  # äºŒåˆ†ç±»
    device_map="auto",
    max_memory={0: "12GB", "cpu": "16GB"}
)
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id
# LoRA é…ç½®
lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, target_modules=["q_proj", "v_proj"], bias="none")

# åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./qlora_llama1b",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    label_names=["labels"],
    metric_for_best_model="accuracy",
    load_best_model_at_end=True,
    report_to="tensorboard",
    logging_dir="./logs",
    logging_steps=1,
    eval_steps=1,
    fp16=True,
    optim="adamw_torch",
)


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    eval_accuracy = accuracy_score(labels, preds)
    return {"accuracy": eval_accuracy}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,

)

trainer.train()

import torch

def classify_email(email):
    email_text = f"Sender: {email['sender']}\nSubject: {email['subject']}\nBody: {email['body']}"

    inputs = tokenizer(email_text, return_tensors="pt", padding=True, truncation=True, max_length=512).to("cuda")

    with torch.no_grad():
        logits = model(**inputs).logits

    predicted_class = torch.argmax(logits, dim=-1).item()
    return "important" if predicted_class == 1 else "non-important"

# æµ‹è¯•
test_email = {
    "sender": "hr@company.com",
    "subject": "Meeting Reminder",
    "body": "Reminder: Team meeting tomorrow at 10 AM."
}
print(f"ğŸ“§ é‚®ä»¶åˆ†ç±»: {classify_email(test_email)}")

from huggingface_hub import login


# ä¸Šä¼ æ¨¡å‹
model.push_to_hub("YULI1234/llama-1b-finetuned")
tokenizer.push_to_hub("YULI1234/llama-1b-finetuned")

eval_results = trainer.evaluate()
print(eval_results)
